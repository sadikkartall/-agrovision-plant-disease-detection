"""
Model EÄŸitimi BetiÄŸi
Bitki hastalÄ±k tespiti iÃ§in transfer learning model eÄŸitimi
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf

# Proje modÃ¼llerini import et
from src.preprocess import DataLoader, DataSplitter
from src.training import DataGenerator, ModelTrainer, TrainingConfig
from src.evaluation import ModelEvaluator


def main():
    """Ana eÄŸitim fonksiyonu"""
    print("ğŸŒ¿ BÄ°TKÄ° HASTALIK TESPÄ°T SÄ°STEMÄ° - MODEL EÄÄ°TÄ°MÄ°")
    print("=" * 70)
    
    # KonfigÃ¼rasyonu yÃ¼kle
    with open('config.json', 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # Veri yolu
    data_path = "data/raw"  # Otomatik olarak ayarla
    
    if not os.path.exists(data_path):
        print(f"âŒ Veri seti bulunamadÄ±: {data_path}")
        return
    
    print(f"ğŸ“ Veri seti yolu: {data_path}")
    
    # Veri hazÄ±rlama
    print("\n1ï¸âƒ£ VERÄ° HAZIRLAMA")
    print("-" * 30)
    
    try:
        data_loader = DataLoader(data_path)
        image_paths, labels, class_names = data_loader.load_dataset()
        
        # DataFrame oluÅŸtur
        df = data_loader.create_dataframe()
        
        # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster
        class_dist = data_loader.get_class_distribution()
        print(f"\nğŸ“Š SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:")
        for class_name, count in class_dist.items():
            print(f"  {class_name}: {count} Ã¶rnek")
        
        # Label encoding kontrolÃ¼ ve dÃ¼zeltme
        print(f"\nğŸ” Label Encoding KontrolÃ¼:")
        print(f"  Numeric label aralÄ±ÄŸÄ±: {df['numeric_label'].min()} - {df['numeric_label'].max()}")
        print(f"  Toplam sÄ±nÄ±f sayÄ±sÄ±: {len(class_names)}")
        
        # Label'larÄ±n 0'dan baÅŸlayÄ±p num_classes-1'e kadar gitmesi gerekiyor
        unique_labels = sorted(df['numeric_label'].unique())
        expected_labels = list(range(len(class_names)))
        
        if unique_labels != expected_labels:
            print(f"  âš ï¸ Label encoding dÃ¼zeltiliyor...")
            from sklearn.preprocessing import LabelEncoder
            le = LabelEncoder()
            df['numeric_label'] = le.fit_transform(df['class_name'])
            print(f"  âœ… Label encoding dÃ¼zeltildi!")
            print(f"  Yeni aralÄ±k: {df['numeric_label'].min()} - {df['numeric_label'].max()}")
        
        # Veri bÃ¶lme
        data_splitter = DataSplitter(train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)
        train_df, val_df, test_df = data_splitter.split_and_organize_dataset(
            df, data_path, "data/splits"
        )
        
        # Split sonrasÄ± label kontrolÃ¼
        print(f"\nğŸ” Split SonrasÄ± Label KontrolÃ¼:")
        print(f"  Train labels: {train_df['numeric_label'].min()} - {train_df['numeric_label'].max()}")
        print(f"  Val labels: {val_df['numeric_label'].min()} - {val_df['numeric_label'].max()}")
        print(f"  Test labels: {test_df['numeric_label'].min()} - {test_df['numeric_label'].max()}")
        
        # Veri bilgilerini kaydet
        with open('data/splits/data_info.json', 'w', encoding='utf-8') as f:
            json.dump({
                'class_names': class_names,
                'train_samples': len(train_df),
                'val_samples': len(val_df),
                'test_samples': len(test_df)
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Veri hazÄ±rlama tamamlandÄ±!")
        print(f"  EÄŸitim: {len(train_df)} Ã¶rnek")
        print(f"  DoÄŸrulama: {len(val_df)} Ã¶rnek")
        print(f"  Test: {len(test_df)} Ã¶rnek")
        
    except Exception as e:
        print(f"âŒ Veri hazÄ±rlama hatasÄ±: {e}")
        return
    
    # EÄŸitim konfigÃ¼rasyonu
    print("\n2ï¸âƒ£ EÄÄ°TÄ°M KONFÄ°GÃœRASYONU")
    print("-" * 30)
    
    training_config = TrainingConfig(
        input_shape=tuple(config['input_shape']),
        num_classes=len(class_names),
        batch_size=config['batch_size'],
        epochs=config['epochs'],
        learning_rate=config['learning_rate'],
        patience=config['patience']
    )
    
    training_config.print_config()
    
    # Veri Ã¼retici
    data_generator = DataGenerator(
        image_size=training_config.input_shape[:2],
        batch_size=training_config.batch_size,
        num_classes=training_config.num_classes,
        augmentation=training_config.augmentation
    )
    
    # Dataset'leri oluÅŸtur
    train_dataset, val_dataset, test_dataset = data_generator.create_train_val_test_datasets(
        train_df, val_df, test_df
    )
    
    # Model eÄŸitici
    trainer = ModelTrainer(training_config.to_dict())
    
    # EÄŸitilecek modeller (sadece stabil olanlar)
    models_to_train = ['mobilenet_v2', 'resnet50']  # Transfer learning modelleri
    print(f"\nğŸ¤– EÄŸitilecek Modeller: {', '.join(models_to_train)}")
    
    # Modelleri eÄŸit
    print("\n3ï¸âƒ£ MODEL EÄÄ°TÄ°MÄ°")
    print("-" * 30)
    
    trained_models = {}
    
    for i, model_name in enumerate(models_to_train, 1):
        print(f"\n[{i}/{len(models_to_train)}] {model_name} modeli eÄŸitiliyor...")
        
        try:
            # Transfer learning iÃ§in optimize edilmiÅŸ learning rate
            # Feature extraction modunda daha dÃ¼ÅŸÃ¼k LR kullanÄ±lÄ±r
            if model_name in ['mobilenet_v2', 'resnet50', 'efficientnet_b0']:
                model_lr = 0.0001  # Feature extraction iÃ§in dÃ¼ÅŸÃ¼k LR
                print(f"  âš™ï¸ Transfer learning modu - Learning rate: {model_lr}")
            else:
                model_lr = training_config.learning_rate
            
            # Model oluÅŸtur
            model = trainer.create_model(
                model_name,
                dropout_rate=training_config.dropout_rate,
                learning_rate=model_lr,
                freeze_base=True  # Base model dondur (feature extraction)
            )
            
            # Model bilgilerini yazdÄ±r
            print(f"  ğŸ“Š Toplam parametre: {model.count_params():,}")
            
            # EÄŸitilebilir parametreleri gÃ¶ster
            trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])
            frozen_params = model.count_params() - trainable_params
            print(f"  ğŸ“Š EÄŸitilebilir: {trainable_params:,}, DondurulmuÅŸ: {frozen_params:,}")
            
            # Modeli eÄŸit
            trained_model = trainer.train_model(
                model, train_dataset, val_dataset, model_name,
                epochs=training_config.epochs
            )
            
            trained_models[model_name] = trained_model
            print(f"  âœ… {model_name} eÄŸitimi tamamlandÄ±!")
            
        except Exception as e:
            print(f"  âŒ {model_name} eÄŸitimi baÅŸarÄ±sÄ±z: {str(e)}")
            continue
    
    if not trained_models:
        print("âŒ HiÃ§bir model baÅŸarÄ±yla eÄŸitilemedi!")
        return
    
    # Model deÄŸerlendirmesi
    print("\n4ï¸âƒ£ MODEL DEÄERLENDÄ°RME")
    print("-" * 30)
    
    evaluator = ModelEvaluator(class_names=class_names, output_dir="reports")
    
    # Modelleri deÄŸerlendir
    evaluation_results = {}
    
    for model_name, model in trained_models.items():
        print(f"\nğŸ” {model_name} deÄŸerlendiriliyor...")
        
        try:
            metrics = evaluator.evaluate_model(
                model, test_dataset, model_name
            )
            evaluation_results[model_name] = metrics
            
            # Temel metrikleri yazdÄ±r
            basic_metrics = metrics.get('basic_metrics', {})
            print(f"  ğŸ“ˆ Test DoÄŸruluÄŸu: {basic_metrics.get('accuracy', 0):.4f}")
            print(f"  ğŸ“ˆ F1-Score: {basic_metrics.get('f1_macro', 0):.4f}")
            
        except Exception as e:
            print(f"  âŒ {model_name} deÄŸerlendirmesi baÅŸarÄ±sÄ±z: {str(e)}")
            continue
    
    # Model karÅŸÄ±laÅŸtÄ±rmasÄ±
    if len(evaluation_results) > 1:
        print("\n5ï¸âƒ£ MODEL KARÅILAÅTIRMASI")
        print("-" * 30)
        
        comparison_df = evaluator.compare_models(trained_models, test_dataset)
        
        # En iyi modeli belirle
        best_model_name = comparison_df.loc[comparison_df['Test Accuracy'].idxmax(), 'Model']
        best_model = trained_models[best_model_name]
        
        print(f"\nğŸ† En iyi model: {best_model_name}")
        print(f"ğŸ“Š Test doÄŸruluÄŸu: {comparison_df['Test Accuracy'].max():.4f}")
        
        # En iyi modeli kaydet
        best_model_path = f"models/saved/best_model_{best_model_name}.h5"
        best_model.save(best_model_path)
        print(f"ğŸ’¾ En iyi model kaydedildi: {best_model_path}")
        
        # KarÅŸÄ±laÅŸtÄ±rma sonuÃ§larÄ±nÄ± kaydet
        comparison_df.to_csv('reports/model_comparison.csv', index=False)
        print(f"ğŸ“Š KarÅŸÄ±laÅŸtÄ±rma sonuÃ§larÄ± kaydedildi: reports/model_comparison.csv")
    
    print("\nâœ… Model eÄŸitimi tamamlandÄ±!")
    print(f"ğŸ“ SonuÃ§lar: reports/ klasÃ¶rÃ¼nde")
    print(f"ğŸ’¾ Modeller: models/saved/ klasÃ¶rÃ¼nde")


if __name__ == "__main__":
    main()
